{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import tensorflow_hub as hub\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:summarizer.preprocessing.cleaner:'pattern' package not found; tag filters are not available for English\n",
      "Using TensorFlow backend.\n",
      "INFO:gensim.utils:loading Word2VecKeyedVectors object from ../embedding/STCWiki/STCWiki_mincount0.model.bin\n",
      "INFO:gensim.utils:loading vectors from ../embedding/STCWiki/STCWiki_mincount0.model.bin.vectors.npy with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute vectors_norm to None\n",
      "INFO:gensim.utils:loaded ../embedding/STCWiki/STCWiki_mincount0.model.bin\n"
     ]
    }
   ],
   "source": [
    "import datahelper\n",
    "datahelper = datahelper.DataHelper(embedding_path=\"../embedding/STCWiki/STCWiki_mincount0.model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_STOPWORDS = False\n",
    "TO_LOWER = True\n",
    "TOKEN_TYPE = 'nltk'\n",
    "EMB = 'stc' # glove or stc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:corpus word2vec:Training data unknown words count: 0\n",
      "INFO:corpus word2vec:Training data max doclen: 150\n",
      "INFO:corpus word2vec:Training data unknown words count: 0\n",
      "INFO:corpus word2vec:Training data max doclen: 150\n",
      "INFO:corpus word2vec:Testing data unknown words count: 0\n",
      "INFO:corpus word2vec:Testing data max doclen: 150\n"
     ]
    }
   ],
   "source": [
    "trainX, trainX_bert, trainND, trainDQ, train_turns, train_masks = datahelper.get_model_train_data(\n",
    "    'train',\n",
    "    TOKEN_TYPE, \n",
    "    REMOVE_STOPWORDS, \n",
    "    TO_LOWER,\n",
    "    EMB,\n",
    "    bert=True,\n",
    ")\n",
    "\n",
    "devX, devX_bert, devND, devDQ, dev_turns, dev_masks = datahelper.get_model_train_data(\n",
    "    'dev',\n",
    "    TOKEN_TYPE, \n",
    "    REMOVE_STOPWORDS, \n",
    "    TO_LOWER,\n",
    "    EMB,\n",
    "    bert=True,\n",
    ")\n",
    "\n",
    "testX, testX_bert, test_turns, test_masks = datahelper.get_model_test_data(\n",
    "    TOKEN_TYPE, \n",
    "    REMOVE_STOPWORDS, \n",
    "    TO_LOWER,\n",
    "    EMB,\n",
    "    bert=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1337, 7, 150, 100), (1337, 7, 1024))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainX_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((335, 7, 150, 100), (335, 7, 1024))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devX.shape, devX_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(trainX_bert, open('trainX_bert_512_sent.p', 'wb'))\n",
    "pickle.dump(devX_bert, open('devX_bert_512_sent.p', 'wb'))\n",
    "pickle.dump(testX_bert, open('testX_bert_512_sent.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = pickle.load(open('trainX_bert.p', 'rb'))\n",
    "devX = pickle.load(open('devX_bert.p', 'rb'))\n",
    "testX = pickle.load(open('testX_bert.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'devX_berts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-740b9d2e0655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevX_berts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevX_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'devX_berts' is not defined"
     ]
    }
   ],
   "source": [
    "devX_berts.shape, devX_bert.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "import os\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hub_model_handle = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sess_config():\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess_config.gpu_options.allow_growth = True\n",
    "    return sess_config\n",
    "\n",
    "def create_tokenizer_from_hub_module(bert_hub_model_handle, sess_config = get_sess_config()):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_hub_model_handle, trainable=True)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session(config=sess_config) as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "    return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer_from_hub_module(bert_hub_model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input_id\n",
    "sent = 'You may try restarting the Wechat app.'\n",
    "bert_tokens = []\n",
    "bert_tokens.append(\"[CLS]\")\n",
    "bert_tokens.extend(tokenizer.tokenize(sent))\n",
    "bert_tokens.append(\"[SEP]\")\n",
    "input_ids = tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "input_mask = [1] * len(input_ids)\n",
    "segment_ids = [0] * len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'u',\n",
       " 'may',\n",
       " 'try',\n",
       " 'restart',\n",
       " '##ing',\n",
       " 'the',\n",
       " 'we',\n",
       " '##cha',\n",
       " '##t',\n",
       " 'app',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 1059, 3270, 4017, 2057, 7507, 4779, 2053, 102]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "bert_inputs = dict(\n",
    "    input_ids=[input_ids],\n",
    "    input_mask=[input_mask],\n",
    "    segment_ids=[segment_ids]\n",
    ")\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(bert_hub_model_handle, trainable=True)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session(config=get_sess_config()) as sess:\n",
    "        bert_outputs = bert_module(bert_inputs, signature=\"tokens\", as_dict=True)\n",
    "        pooled_output = bert_outputs[\"pooled_output\"]\n",
    "        sequence_output = bert_outputs[\"sequence_output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train_corpus = pickle.load(open(\"train_corpus.p\", \"rb\"))\n",
    "dev_corpus = pickle.load(open(\"dev_corpus.p\", \"rb\"))\n",
    "test_corpus = pickle.load(open(\"test_corpus.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = [train_corpus[i][2] for i in range(len(train_corpus))]\n",
    "devX = [dev_corpus[i][2] for i in range(len(dev_corpus))]\n",
    "testX = [train_corpus[i][2] for i in range(len(test_corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 150\n",
    "max_sent = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_tokens(utterance):\n",
    "    global max_len\n",
    "    bert_tokens = []\n",
    "    bert_tokens.append(\"[CLS]\")\n",
    "    bert_tokens.extend(tokenizer.tokenize(utterance))\n",
    "    bert_tokens.append(\"[SEP]\")\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_corpus_to_bert_tokens(X):\n",
    "    input_ids = []\n",
    "    for dialogue in X:\n",
    "        utterances = []\n",
    "        for utterance in dialogue:\n",
    "            utterances.append(get_bert_tokens(utterance))\n",
    "        input_ids.append(utterances.copy())\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(X):\n",
    "    max_len = 0\n",
    "    count = 0\n",
    "    for dialogue in X:\n",
    "        for utterance in dialogue:\n",
    "            count = count + 1 if len(utterance) > 150 else count\n",
    "            max_len = max(len(utterance), max_len)\n",
    "    print(count / len(X))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maskid_seqmentid(X):\n",
    "    global max_len\n",
    "    dialogue_masks = []\n",
    "    dialogue_segids = []\n",
    "    for dialogue in X:\n",
    "        utterance_masks = []\n",
    "        utterance_segids = []\n",
    "        for utterance in dialogue:\n",
    "            seqlen = min(max_len, len(utterance))\n",
    "            input_mask = [1] * seqlen + [0] * (max_len - seqlen)\n",
    "            seg_id = [0] * max_len\n",
    "            utterance_masks.append(input_mask.copy())\n",
    "            utterance_segids.append(seg_id.copy())\n",
    "        dialogue_masks.append(utterance_masks.copy())\n",
    "        dialogue_segids.append(utterance_segids.copy())\n",
    "    return dialogue_masks, dialogue_segids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_input_ids = convert_corpus_to_bert_tokens(trainX)\n",
    "devX_input_ids = convert_corpus_to_bert_tokens(devX)\n",
    "testX_input_ids = convert_corpus_to_bert_tokens(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(get_max_len(trainX_input_ids), get_max_len(devX_input_ids), get_max_len(testX_input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_input_masks, trainX_segment_ids = get_maskid_seqmentid(trainX_input_ids)\n",
    "devX_input_masks, devX_segment_ids = get_maskid_seqmentid(devX_input_ids)\n",
    "testX_input_masks, testX_segment_ids = get_maskid_seqmentid(testX_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150, 150)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX_input_ids[0][0]), len(trainX_input_masks[0][0]), len(trainX_segment_ids[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_padding(input_ids, input_masks, segment_ids):\n",
    "    global max_sent\n",
    "    padding = [0] * max_len\n",
    "    for i in range(len(input_ids)):\n",
    "        while len(input_ids[i]) < max_sent:\n",
    "            input_ids[i].append(padding)\n",
    "            \n",
    "        while len(input_masks[i]) < max_sent:\n",
    "            input_masks[i].append(padding)\n",
    "        \n",
    "        while len(segment_ids[i]) < max_sent:\n",
    "            segment_ids[i].append(padding)\n",
    "            \n",
    "        for j in range(len(input_ids[i])):\n",
    "            seqlen = len(input_ids[i][j])\n",
    "            if seqlen > max_len:\n",
    "                input_ids[i][j] = input_ids[i][j][:max_len]\n",
    "            if seqlen < max_len:\n",
    "                input_ids[i][j].extend([0] * (max_len - seqlen))\n",
    "                \n",
    "    return input_ids, input_masks, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_input_ids_pad, trainX_input_masks_pad, trainX_segment_ids_pad = bert_padding(\n",
    "    trainX_input_ids, \n",
    "    trainX_input_masks, \n",
    "    trainX_segment_ids,\n",
    ")\n",
    "\n",
    "devX_input_ids_pad, devX_input_masks_pad, devX_segment_ids_pad = bert_padding(\n",
    "    devX_input_ids, \n",
    "    devX_input_masks, \n",
    "    devX_segment_ids,\n",
    ")\n",
    "\n",
    "testX_input_ids_pad, testX_input_masks_pad, testX_segment_ids_pad = bert_padding(\n",
    "    testX_input_ids, \n",
    "    testX_input_masks, \n",
    "    testX_segment_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dialogue in testX_segment_ids_pad:\n",
    "    assert len(dialogue) == 7\n",
    "    for utterance in dialogue:\n",
    "        assert len(utterance) == 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7, 150), (7, 150), (7, 150))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.asarray(trainX_input_ids_pad[0]).shape, np.asarray(trainX_input_masks_pad[0]).shape, np.asarray(trainX_segment_ids_pad[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstacked_input_ids = []\n",
    "unstacked_input_mask = []\n",
    "unstacked_segment_ids = []\n",
    "\n",
    "for _id, mask, segid in zip(trainX_input_ids_pad[0:3], trainX_input_masks_pad[0:3], trainX_segment_ids_pad[0:3]):\n",
    "    unstacked_input_ids.extend(_id)\n",
    "    unstacked_input_mask.extend(mask)\n",
    "    unstacked_segment_ids.extend(segid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "bert_inputs = dict(\n",
    "    input_ids=trainX_input_ids_pad[0],\n",
    "    input_mask=trainX_input_masks_pad[0],\n",
    "    segment_ids=trainX_segment_ids_pad[0],\n",
    ")\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(bert_hub_model_handle, trainable=True)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session(config=get_sess_config()) as sess:\n",
    "        bert_outputs = bert_module(bert_inputs, signature=\"tokens\", as_dict=True)\n",
    "        pooled_output = bert_outputs[\"pooled_output\"]\n",
    "        sequence_output = bert_outputs[\"sequence_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(7), Dimension(768)])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(trainX_input_ids_pad, open('trainX_input_ids.p', 'wb'))\n",
    "pickle.dump(devX_input_ids_pad, open('devX_input_ids.p', 'wb'))\n",
    "pickle.dump(testX_input_ids_pad, open('testX_input_ids.p', 'wb'))\n",
    "pickle.dump(trainX_input_masks_pad, open('trainX_input_masks.p', 'wb'))\n",
    "pickle.dump(devX_input_masks_pad, open('devX_input_masks.p', 'wb'))\n",
    "pickle.dump(testX_input_masks_pad, open('testX_input_masks.p', 'wb'))\n",
    "pickle.dump(trainX_segment_ids_pad, open('trainX_segment_ids.p', 'wb'))\n",
    "pickle.dump(devX_segment_ids_pad, open('devX_segment_ids.p', 'wb'))\n",
    "pickle.dump(testX_segment_ids_pad, open('testX_segment_ids.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
