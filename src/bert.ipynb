{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datahelper\n",
    "datahelper = datahelper.DataHelper(embedding_path=\"../embedding/STCWiki/STCWiki_mincount0.model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_STOPWORDS = False\n",
    "TO_LOWER = True\n",
    "TOKEN_TYPE = 'nltk'\n",
    "EMB = 'stc' # glove or stc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainX_bert, trainND, trainDQ, train_turns, train_masks = datahelper.get_model_train_data(\n",
    "    'train',\n",
    "    TOKEN_TYPE, \n",
    "    REMOVE_STOPWORDS, \n",
    "    TO_LOWER,\n",
    "    EMB,\n",
    "    bert=True,\n",
    ")\n",
    "\n",
    "devX, devX_bert, devND, devDQ, dev_turns, dev_masks = datahelper.get_model_train_data(\n",
    "    'dev',\n",
    "    TOKEN_TYPE, \n",
    "    REMOVE_STOPWORDS, \n",
    "    TO_LOWER,\n",
    "    EMB,\n",
    "    bert=True,\n",
    ")\n",
    "\n",
    "testX, testX_bert, test_turns, test_masks = datahelper.get_model_test_data(\n",
    "    TOKEN_TYPE, \n",
    "    REMOVE_STOPWORDS, \n",
    "    TO_LOWER,\n",
    "    EMB,\n",
    "    bert=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape, trainX_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devX.shape, devX_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(trainX_bert, open('trainX_bert_512.p', 'wb'))\n",
    "pickle.dump(devX_bert, open('devX_bert_512.p', 'wb'))\n",
    "pickle.dump(testX_bert, open('testX_bert_512.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = pickle.load(open('trainX_bert.p', 'rb'))\n",
    "devX = pickle.load(open('devX_bert.p', 'rb'))\n",
    "testX = pickle.load(open('testX_bert.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devX_berts.shape, devX_bert.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sess_config():\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess_config.gpu_options.allow_growth = True\n",
    "    return sess_config\n",
    "\n",
    "def create_tokenizer_from_hub_module(bert_module, sess_config=get_sess_config()):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_hub_model_handle = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_hub_model_handle, trainable=True)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session(config=sess_config) as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                  tokenization_info[\"do_lower_case\"]])\n",
    "    return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "bert_module = get_bert_module()\n",
    "tokenizer = create_tokenizer_from_hub_module(bert_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input_id\n",
    "sent = 'i whaat wechatt no'\n",
    "bert_tokens = []\n",
    "bert_tokens.append(\"[CLS]\")\n",
    "bert_tokens.extend(tokenizer.tokenize(sent))\n",
    "bert_tokens.append(\"[SEP]\")\n",
    "input_ids = tokenizer.convert_tokens_to_ids(bert_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'w', '##ha', '##at', 'we', '##cha', '##tt', 'no', '[SEP]']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 1059, 3270, 4017, 2057, 7507, 4779, 2053, 102]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = dict(\n",
    "    input_ids=input_ids,\n",
    ")\n",
    "\n",
    "bert_outputs = bert_module(\n",
    "    signature=\"tokens\", \n",
    "    as_dict=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
