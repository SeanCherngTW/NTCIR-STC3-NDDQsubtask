{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import tensorflow_hub as hub\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datahelper\n",
    "datahelper = datahelper.DataHelper(embedding_path=\"../embedding/STCWiki/STCWiki_mincount0.model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_STOPWORDS = False\n",
    "TO_LOWER = True\n",
    "TOKEN_TYPE = 'nltk'\n",
    "EMB = 'stc' # glove or stc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainX_bert, trainND, trainDQ, train_turns, train_masks = datahelper.get_model_train_data(\n",
    "    'train',\n",
    "    TOKEN_TYPE, \n",
    "    REMOVE_STOPWORDS, \n",
    "    TO_LOWER,\n",
    "    EMB,\n",
    "    bert=True,\n",
    ")\n",
    "\n",
    "devX, devX_bert, devND, devDQ, dev_turns, dev_masks = datahelper.get_model_train_data(\n",
    "    'dev',\n",
    "    TOKEN_TYPE, \n",
    "    REMOVE_STOPWORDS, \n",
    "    TO_LOWER,\n",
    "    EMB,\n",
    "    bert=True,\n",
    ")\n",
    "\n",
    "testX, testX_bert, test_turns, test_masks = datahelper.get_model_test_data(\n",
    "    TOKEN_TYPE, \n",
    "    REMOVE_STOPWORDS, \n",
    "    TO_LOWER,\n",
    "    EMB,\n",
    "    bert=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape, trainX_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devX.shape, devX_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(trainX_bert, open('trainX_bert_512_sent.p', 'wb'))\n",
    "pickle.dump(devX_bert, open('devX_bert_512_sent.p', 'wb'))\n",
    "pickle.dump(testX_bert, open('testX_bert_512_sent.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = pickle.load(open('trainX_bert.p', 'rb'))\n",
    "devX = pickle.load(open('devX_bert.p', 'rb'))\n",
    "testX = pickle.load(open('testX_bert.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devX_berts.shape, devX_bert.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "import os\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hub_model_handle = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sess_config():\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess_config.gpu_options.allow_growth = True\n",
    "    return sess_config\n",
    "\n",
    "def create_tokenizer_from_hub_module(bert_hub_model_handle, sess_config = get_sess_config()):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_hub_model_handle, trainable=True)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session(config=sess_config) as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "    return tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer_from_hub_module(bert_hub_model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input_id\n",
    "sent = 'You may try restarting the Wechat app.'\n",
    "bert_tokens = []\n",
    "bert_tokens.append(\"[CLS]\")\n",
    "bert_tokens.extend(tokenizer.tokenize(sent))\n",
    "bert_tokens.append(\"[SEP]\")\n",
    "input_ids = tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "input_mask = [1] * len(input_ids)\n",
    "segment_ids = [0] * len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = dict(\n",
    "    input_ids=[input_ids],\n",
    "    input_mask=[input_mask],\n",
    "    segment_ids=[segment_ids]\n",
    ")\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(bert_hub_model_handle, trainable=True)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session(config=get_sess_config()) as sess:\n",
    "        bert_outputs = bert_module(bert_inputs, signature=\"tokens\", as_dict=True)\n",
    "        pooled_output = bert_outputs[\"pooled_output\"]\n",
    "        sequence_output = bert_outputs[\"sequence_output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train_corpus = pickle.load(open(\"train_corpus.p\", \"rb\"))\n",
    "dev_corpus = pickle.load(open(\"dev_corpus.p\", \"rb\"))\n",
    "test_corpus = pickle.load(open(\"test_corpus.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = [train_corpus[i][2] for i in range(len(train_corpus))]\n",
    "devX = [dev_corpus[i][2] for i in range(len(dev_corpus))]\n",
    "testX = [train_corpus[i][2] for i in range(len(test_corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 150\n",
    "max_sent = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_tokens(utterance):\n",
    "    global max_len\n",
    "    bert_tokens = []\n",
    "    bert_tokens.append(\"[CLS]\")\n",
    "    bert_tokens.extend(tokenizer.tokenize(utterance))\n",
    "    bert_tokens.append(\"[SEP]\")\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_corpus_to_bert_tokens(X):\n",
    "    input_ids = []\n",
    "    for dialogue in X:\n",
    "        utterances = []\n",
    "        for utterance in dialogue:\n",
    "            utterances.append(get_bert_tokens(utterance))\n",
    "        input_ids.append(utterances.copy())\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(X):\n",
    "    max_len = 0\n",
    "    count = 0\n",
    "    for dialogue in X:\n",
    "        for utterance in dialogue:\n",
    "            count = count + 1 if len(utterance) > 150 else count\n",
    "            max_len = max(len(utterance), max_len)\n",
    "    print(count / len(X))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maskid_seqmentid(X):\n",
    "    global max_len\n",
    "    dialogue_masks = []\n",
    "    dialogue_segids = []\n",
    "    for dialogue in X:\n",
    "        utterance_masks = []\n",
    "        utterance_segids = []\n",
    "        for utterance in dialogue:\n",
    "            seqlen = min(max_len, len(utterance))\n",
    "            input_mask = [1] * seqlen + [0] * (max_len - seqlen)\n",
    "            seg_id = [0] * max_len\n",
    "            utterance_masks.append(input_mask.copy())\n",
    "            utterance_segids.append(seg_id.copy())\n",
    "        dialogue_masks.append(utterance_masks.copy())\n",
    "        dialogue_segids.append(utterance_segids.copy())\n",
    "    return dialogue_masks, dialogue_segids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_input_ids = convert_corpus_to_bert_tokens(trainX)\n",
    "devX_input_ids = convert_corpus_to_bert_tokens(devX)\n",
    "testX_input_ids = convert_corpus_to_bert_tokens(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max(get_max_len(trainX_input_ids), get_max_len(devX_input_ids), get_max_len(testX_input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_input_masks, trainX_segment_ids = get_maskid_seqmentid(trainX_input_ids)\n",
    "devX_input_masks, devX_segment_ids = get_maskid_seqmentid(devX_input_ids)\n",
    "testX_input_masks, testX_segment_ids = get_maskid_seqmentid(testX_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainX_input_ids[0][0]), len(trainX_input_masks[0][0]), len(trainX_segment_ids[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_padding(input_ids, input_masks, segment_ids):\n",
    "    global max_sent\n",
    "    padding = [0] * max_len\n",
    "    for i in range(len(input_ids)):\n",
    "        while len(input_ids[i]) < max_sent:\n",
    "            input_ids[i].append(padding)\n",
    "            \n",
    "        while len(input_masks[i]) < max_sent:\n",
    "            input_masks[i].append(padding)\n",
    "        \n",
    "        while len(segment_ids[i]) < max_sent:\n",
    "            segment_ids[i].append(padding)\n",
    "            \n",
    "        for j in range(len(input_ids[i])):\n",
    "            seqlen = len(input_ids[i][j])\n",
    "            if seqlen > max_len:\n",
    "                input_ids[i][j] = input_ids[i][j][:max_len]\n",
    "            if seqlen < max_len:\n",
    "                input_ids[i][j].extend([0] * (max_len - seqlen))\n",
    "                \n",
    "    return input_ids, input_masks, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_input_ids_pad, trainX_input_masks_pad, trainX_segment_ids_pad = bert_padding(\n",
    "    trainX_input_ids, \n",
    "    trainX_input_masks, \n",
    "    trainX_segment_ids,\n",
    ")\n",
    "\n",
    "devX_input_ids_pad, devX_input_masks_pad, devX_segment_ids_pad = bert_padding(\n",
    "    devX_input_ids, \n",
    "    devX_input_masks, \n",
    "    devX_segment_ids,\n",
    ")\n",
    "\n",
    "testX_input_ids_pad, testX_input_masks_pad, testX_segment_ids_pad = bert_padding(\n",
    "    testX_input_ids, \n",
    "    testX_input_masks, \n",
    "    testX_segment_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dialogue in testX_segment_ids_pad:\n",
    "    assert len(dialogue) == 7\n",
    "    for utterance in dialogue:\n",
    "        assert len(utterance) == 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.asarray(trainX_input_ids_pad[0]).shape, np.asarray(trainX_input_masks_pad[0]).shape, np.asarray(trainX_segment_ids_pad[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstacked_input_ids = []\n",
    "unstacked_input_mask = []\n",
    "unstacked_segment_ids = []\n",
    "\n",
    "for _id, mask, segid in zip(trainX_input_ids_pad[0:3], trainX_input_masks_pad[0:3], trainX_segment_ids_pad[0:3]):\n",
    "    unstacked_input_ids.extend(_id)\n",
    "    unstacked_input_mask.extend(mask)\n",
    "    unstacked_segment_ids.extend(segid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = dict(\n",
    "    input_ids=trainX_input_ids_pad[0],\n",
    "    input_mask=trainX_input_masks_pad[0],\n",
    "    segment_ids=trainX_segment_ids_pad[0],\n",
    ")\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(bert_hub_model_handle, trainable=True)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session(config=get_sess_config()) as sess:\n",
    "        bert_outputs = bert_module(bert_inputs, signature=\"tokens\", as_dict=True)\n",
    "        pooled_output = bert_outputs[\"pooled_output\"]\n",
    "        sequence_output = bert_outputs[\"sequence_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(trainX_input_ids_pad, open('trainX_input_ids.p', 'wb'))\n",
    "pickle.dump(devX_input_ids_pad, open('devX_input_ids.p', 'wb'))\n",
    "pickle.dump(testX_input_ids_pad, open('testX_input_ids.p', 'wb'))\n",
    "pickle.dump(trainX_input_masks_pad, open('trainX_input_masks.p', 'wb'))\n",
    "pickle.dump(devX_input_masks_pad, open('devX_input_masks.p', 'wb'))\n",
    "pickle.dump(testX_input_masks_pad, open('testX_input_masks.p', 'wb'))\n",
    "pickle.dump(trainX_segment_ids_pad, open('trainX_segment_ids.p', 'wb'))\n",
    "pickle.dump(devX_segment_ids_pad, open('devX_segment_ids.p', 'wb'))\n",
    "pickle.dump(testX_segment_ids_pad, open('testX_segment_ids.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "trainX_input_ids = pickle.load(open('trainX_input_ids.p', 'rb'))\n",
    "trainX_input_masks = pickle.load(open('trainX_input_masks.p', 'rb'))\n",
    "trainX_segment_ids = pickle.load(open('trainX_segment_ids.p', 'rb'))\n",
    "devX_input_ids = pickle.load(open('devX_input_ids.p', 'rb'))\n",
    "devX_input_masks = pickle.load(open('devX_input_masks.p', 'rb'))\n",
    "devX_segment_ids = pickle.load(open('devX_segment_ids.p', 'rb'))\n",
    "testX_input_ids = pickle.load(open('testX_input_ids.p', 'rb'))\n",
    "testX_input_masks = pickle.load(open('testX_input_masks.p', 'rb'))\n",
    "testX_segment_ids = pickle.load(open('testX_segment_ids.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_module = hub.Module(\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getshape(l):\n",
    "    return np.array(l).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = trainX_input_ids[0:3]\n",
    "input_mask = trainX_input_masks[0:3]\n",
    "segment_ids = trainX_segment_ids[0:3]\n",
    "print(getshape(input_ids), getshape(input_mask), getshape(segment_ids))\n",
    "\n",
    "for idx, (dialog_ids, dialog_masks, dialog_segids) in enumerate(zip(input_ids, input_mask, segment_ids)):\n",
    "    for _id, mask, segid in zip(dialog_ids, dialog_masks, dialog_segids):\n",
    "        print(getshape(_id), getshape(mask), getshape(segid))\n",
    "        print(getshape([_id]))\n",
    "        bert_inputs = dict(\n",
    "            input_ids=[_id],\n",
    "            input_mask=[mask],\n",
    "            segment_ids=[segid],\n",
    "        )\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_outputs = bert_module(bert_inputs, signature=\"tokens\", as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output = bert_outputs[\"pooled_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
