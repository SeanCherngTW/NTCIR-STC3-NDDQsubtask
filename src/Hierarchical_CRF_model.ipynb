{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import timeit\n",
    "import random\n",
    "import param\n",
    "import shutil\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import stctrain\n",
    "import datahelper\n",
    "import stctokenizer\n",
    "import nuggetdetection as ND\n",
    "import dialogquality as DQ\n",
    "import dialogquality_ndfeature as DQNDF\n",
    "import stcevaluation as STCE\n",
    "\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doclen = param.doclen\n",
    "embsize = param.embsize\n",
    "max_sent = param.max_sent\n",
    "NDclasses = param.NDclasses\n",
    "DQclasses = param.DQclasses\n",
    "sentembsize = param.sentembsize\n",
    "\n",
    "REMOVE_STOPWORDS = False\n",
    "TO_LOWER = True\n",
    "TOKEN_TYPE = 'nltk'\n",
    "EMB = 'stc' # glove or stc\n",
    "\n",
    "datahelper = datahelper.DataHelper(embedding_path=\"../embedding/STCWiki/STCWiki_mincount0.model.bin\")\n",
    "stctokenizer = stctokenizer.STCTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = datahelper.prepare_word_embedding_corpus(\n",
    "#     '../data/text8', \n",
    "#     TOKEN_TYPE, \n",
    "#     REMOVE_STOPWORDS, \n",
    "#     TO_LOWER,\n",
    "# )\n",
    "\n",
    "# wordemb_model = Word2Vec(corpus, size=100, min_count=0, workers=4, iter=30, sg=1, window=5)\n",
    "# word_vectors = wordemb_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors.save(\"../embedding/STCWiki/STCWiki_mincount0.model.bin\")\n",
    "# datahelper.set_word_vectors(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Corpus & Prepare data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data():\n",
    "trainX, _, trainND, trainDQ, train_turns, train_masks = datahelper.get_model_train_data(\n",
    "    'train',\n",
    "    TOKEN_TYPE, \n",
    "    REMOVE_STOPWORDS, \n",
    "    TO_LOWER,\n",
    "    EMB,\n",
    "    bert=False,\n",
    ")\n",
    "\n",
    "devX, _, devND, devDQ, dev_turns, dev_masks = datahelper.get_model_train_data(\n",
    "    'dev',\n",
    "    TOKEN_TYPE, \n",
    "    REMOVE_STOPWORDS, \n",
    "    TO_LOWER,\n",
    "    EMB,\n",
    "    bert=False,\n",
    ")\n",
    "\n",
    "testX, _, test_turns, test_masks = datahelper.get_model_test_data(\n",
    "    TOKEN_TYPE, \n",
    "    REMOVE_STOPWORDS, \n",
    "    TO_LOWER,\n",
    "    EMB,\n",
    "    bert=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.asarray([0., 0., 0., 0.5, 0.5, 0., 0.])\n",
    "np.max(a)\n",
    "# (a==0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "MultiAnsUtt = namedtuple(\"MultiAnsUtt\", ['uttidx', 'secondans'])\n",
    "\n",
    "def highest_label_idx(prob):\n",
    "    highest = np.max(prob)\n",
    "    return np.where(prob==highest)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "aa = highest_label_idx(trainND[0][6])\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_second_ans(uttidx, second_ansidx, CRFX, CRFND, CRFTurns, CRFMasks, CRFDialogND, dialogX, dialogTurn, dialogMask):\n",
    "    global max_sent\n",
    "    CRF_label = np.asarray([0.] * max_sent)\n",
    "    CRF_label[second_ansidx] = 1.\n",
    "    CRFDialogND[uttidx] = CRF_label\n",
    "    CRFX.append(dialogX)\n",
    "    CRFND.append(CRFDialogND.copy())\n",
    "    CRFTurns.append(dialogTurn)\n",
    "    CRFMasks.append(dialogMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCRF(X, ND, turns, masks):\n",
    "    CRFX = []\n",
    "    CRFND = []\n",
    "    CRFTurns = []\n",
    "    CRFMasks = []\n",
    "    for dialogidx, (dialogX, dialogND, dialogTurn, dialogMask) in enumerate(zip(X, ND, turns, masks)):\n",
    "        multi_ans_uttidx = []\n",
    "        CRFDialogND = []\n",
    "\n",
    "        for uttidx, (uttX, uttND, uttMask) in enumerate(zip(dialogX, dialogND, dialogMask)):\n",
    "            ans_idx = highest_label_idx(uttND)\n",
    "            num_of_ans = len(ans_idx)\n",
    "            CRF_label = np.asarray([0.] * max_sent)\n",
    "            if num_of_ans == 7:  # all zero\n",
    "                CRFDialogND.append(CRF_label)\n",
    "            elif num_of_ans == 1:  # one ans\n",
    "                CRF_label[ans_idx[0]] = 1.\n",
    "                CRFDialogND.append(CRF_label)\n",
    "            elif num_of_ans == 2:  # two ans\n",
    "                CRF_label[ans_idx[0]] = 1.\n",
    "                CRFDialogND.append(CRF_label)\n",
    "                multi_ans_uttidx.append(MultiAnsUtt(uttidx, ans_idx[1]))  # save the second ans and add later\n",
    "            else:\n",
    "                assert False, 'ND ans with more than 2'\n",
    "\n",
    "        CRFX.append(dialogX)\n",
    "        CRFND.append(CRFDialogND.copy())\n",
    "        CRFTurns.append(dialogTurn)\n",
    "        CRFMasks.append(dialogMask)\n",
    "\n",
    "        if len(multi_ans_uttidx) == 1:\n",
    "            uttidx, second_ansidx = multi_ans_uttidx[0]\n",
    "            add_second_ans(uttidx, second_ansidx, CRFX, CRFND, CRFTurns, CRFMasks, CRFDialogND.copy(), dialogX, dialogTurn, dialogMask)\n",
    "\n",
    "        elif len(multi_ans_uttidx) == 2:\n",
    "            uttidx0, second_ansidx0 = multi_ans_uttidx[0]\n",
    "            uttidx1, second_ansidx1 = multi_ans_uttidx[1]\n",
    "\n",
    "            # 1st ans & 1st ans is already in the final list\n",
    "\n",
    "            # 1st ans & 2nd ans\n",
    "            add_second_ans(uttidx1, second_ansidx1, CRFX, CRFND, CRFTurns, CRFMasks, CRFDialogND.copy(), dialogX, dialogTurn, dialogMask)\n",
    "            # 2nd ans & 1st ans\n",
    "            add_second_ans(uttidx0, second_ansidx0, CRFX, CRFND, CRFTurns, CRFMasks, CRFDialogND, dialogX, dialogTurn, dialogMask)\n",
    "            # 2nd ans & 2nd ans\n",
    "            add_second_ans(uttidx1, second_ansidx1, CRFX, CRFND, CRFTurns, CRFMasks, CRFDialogND, dialogX, dialogTurn, dialogMask)\n",
    "    \n",
    "    assert len(CRFX) == len(CRFND) == len(CRFTurns) == len(CRFMasks)\n",
    "    return CRFX, CRFND, CRFTurns, CRFMasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 1\n",
      "15 5\n",
      "23 1\n",
      "26 2\n",
      "68 2\n",
      "80 3\n",
      "87 5\n",
      "89 3\n",
      "98 3\n",
      "101 1\n",
      "101 2\n",
      "107 1\n",
      "122 4\n",
      "148 1\n",
      "166 2\n",
      "177 0\n",
      "189 4\n",
      "191 1\n",
      "202 5\n",
      "205 1\n",
      "241 2\n",
      "263 3\n",
      "265 4\n",
      "269 2\n",
      "298 4\n",
      "319 1\n",
      "319 2\n",
      "327 3\n",
      "333 4\n",
      "340 2\n",
      "356 3\n",
      "359 3\n",
      "361 3\n",
      "375 1\n",
      "377 1\n",
      "392 3\n",
      "401 1\n",
      "408 4\n",
      "416 2\n",
      "418 3\n",
      "431 1\n",
      "438 3\n",
      "448 5\n",
      "471 0\n",
      "495 3\n",
      "503 3\n",
      "527 1\n",
      "537 2\n",
      "545 5\n",
      "559 1\n",
      "577 4\n",
      "579 1\n",
      "580 1\n",
      "580 2\n",
      "599 5\n",
      "615 3\n",
      "637 3\n",
      "650 1\n",
      "664 3\n",
      "676 5\n",
      "679 1\n",
      "686 2\n",
      "688 3\n",
      "710 2\n",
      "712 4\n",
      "719 1\n",
      "725 3\n",
      "742 1\n",
      "747 3\n",
      "750 3\n",
      "756 0\n",
      "756 1\n",
      "774 1\n",
      "790 2\n",
      "791 1\n",
      "793 5\n",
      "812 3\n",
      "816 3\n",
      "823 1\n",
      "834 2\n",
      "835 5\n",
      "867 5\n",
      "869 3\n",
      "903 1\n",
      "921 1\n",
      "929 4\n",
      "937 2\n",
      "944 2\n",
      "947 1\n",
      "961 5\n",
      "978 0\n",
      "978 1\n",
      "983 2\n",
      "984 1\n",
      "1000 2\n",
      "1002 1\n",
      "1004 2\n",
      "1040 1\n",
      "1042 1\n",
      "1051 4\n",
      "1053 3\n",
      "1061 1\n",
      "1065 6\n",
      "1089 1\n",
      "1101 6\n",
      "1107 4\n",
      "1114 0\n",
      "1125 2\n",
      "1128 1\n",
      "1140 1\n",
      "1162 2\n",
      "1168 1\n",
      "1171 5\n",
      "1183 2\n",
      "1187 2\n",
      "1220 2\n",
      "1231 5\n",
      "1231 6\n",
      "1243 3\n",
      "1267 1\n",
      "1276 5\n",
      "1302 3\n",
      "1302 4\n"
     ]
    }
   ],
   "source": [
    "CRFtrainX, CRFtrainND, CRFtrain_turns, CRFtrain_masks = convertCRF(trainX, trainND, train_turns, train_masks)\n",
    "# CRFdevX, CRFdevND, CRFdev_turns, CRFdev_masks = convertCRF(devX, devND, dev_turns, dev_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on 1 dialogue with 2 ans in 1 utterance\n",
      "array([[ 0.        ,  0.        ,  0.2631579 ,  0.7368421 ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.15789473,\n",
      "         0.42105263,  0.42105263],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ]], dtype=float32)\n",
      "[array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      "[array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      "\n",
      "array([[ 0.        ,  0.        ,  0.10526316,  0.89473683,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.89473683,  0.10526316],\n",
      "       [ 0.        ,  0.89473683,  0.10526316,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.36842105,\n",
      "         0.52631581,  0.10526316],\n",
      "       [ 0.05263158,  0.63157892,  0.31578946,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.05263158,\n",
      "         0.47368422,  0.47368422],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ]], dtype=float32)\n",
      "[array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.]),\n",
      " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.]),\n",
      " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      "[array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.]),\n",
      " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.]),\n",
      " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      "\n",
      "Test on 1 dialogue with 2 ans in 2 utterances\n",
      "array([[ 0.        ,  0.10526316,  0.47368422,  0.42105263,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.05263158,\n",
      "         0.47368422,  0.47368422],\n",
      "       [ 0.05263158,  0.47368422,  0.47368422,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.42105263,  0.57894737],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
      "         0.        ,  0.        ]], dtype=float32)\n",
      "[array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.]),\n",
      " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      "[array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.]),\n",
      " array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      "[array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
      " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      "[array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
      " array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
      " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      "PASS\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print('Test on 1 dialogue with 2 ans in 1 utterance')\n",
    "i = 14\n",
    "CRFi = 14\n",
    "pprint(trainND[i])\n",
    "pprint(CRFtrainND[CRFi])\n",
    "pprint(CRFtrainND[CRFi+1])\n",
    "assert(CRFtrainX[CRFi].all() == CRFtrainX[CRFi+1].all())\n",
    "assert(CRFtrain_turns[CRFi] == CRFtrain_turns[CRFi+1])\n",
    "assert(CRFtrain_masks[CRFi].all() == CRFtrain_masks[CRFi+1].all())\n",
    "print()\n",
    "i = 15\n",
    "CRFi = 16\n",
    "pprint(trainND[i])\n",
    "pprint(CRFtrainND[CRFi])\n",
    "pprint(CRFtrainND[CRFi+1])\n",
    "print()\n",
    "\n",
    "print('Test on 1 dialogue with 2 ans in 2 utterances')\n",
    "i = 101\n",
    "pprint(trainND[i])\n",
    "CRFi = 110\n",
    "pprint(CRFtrainND[CRFi])\n",
    "pprint(CRFtrainND[CRFi+1])\n",
    "pprint(CRFtrainND[CRFi+2])\n",
    "pprint(CRFtrainND[CRFi+3])\n",
    "assert(CRFtrainX[CRFi].all() == CRFtrainX[CRFi+1].all() == CRFtrainX[CRFi+2].all() == CRFtrainX[CRFi+3].all())\n",
    "assert(CRFtrain_turns[CRFi] == CRFtrain_turns[CRFi+1] == CRFtrain_turns[CRFi+2] == CRFtrain_turns[CRFi+3])\n",
    "assert(CRFtrain_masks[CRFi].all() == CRFtrain_masks[CRFi+1].all() == CRFtrain_masks[CRFi+2].all() == CRFtrain_masks[CRFi+3].all())\n",
    "\n",
    "print('PASS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRFtrain_turns[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIDs = datahelper.testIDs\n",
    "trainDQA = [item['A'] for item in trainDQ]\n",
    "trainDQS = [item['S'] for item in trainDQ]\n",
    "trainDQE = [item['E'] for item in trainDQ]\n",
    "devDQA = [item['A'] for item in devDQ]\n",
    "devDQS = [item['S'] for item in devDQ]\n",
    "devDQE = [item['E'] for item in devDQ]\n",
    "\n",
    "dataND = [trainX, trainND, train_turns, train_masks, devX, devND, dev_turns, dev_masks, testX, test_turns, test_masks]\n",
    "dataDQA = [trainX, trainDQA, train_turns, devX, devDQA, dev_turns, testX, test_turns]\n",
    "dataDQS = [trainX, trainDQS, train_turns, devX, devDQS, dev_turns, testX, test_turns]\n",
    "dataDQE = [trainX, trainDQE, train_turns, devX, devDQE, dev_turns, testX, test_turns]\n",
    "\n",
    "dataDQA_NDF = [trainX, trainDQA, train_turns, trainND, devX, devDQA, dev_turns, devND, testX, test_turns]\n",
    "dataDQE_NDF = [trainX, trainDQE, train_turns, trainND, devX, devDQE, dev_turns, devND, testX, test_turns]\n",
    "dataDQS_NDF = [trainX, trainDQS, train_turns, trainND, devX, devDQS, dev_turns, devND, testX, test_turns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = 3\n",
    "fixed_paramsND  = {\n",
    "    'epoch':100, \n",
    "    'early_stopping':es, \n",
    "    'batch_size':30,\n",
    "    'lr':5e-4,\n",
    "    'kp':1, \n",
    "    'hiddens':1024, \n",
    "    'Fsize':[2,3], \n",
    "    'gating':False, \n",
    "    'bn':True, \n",
    "    'method':ND.CNNRNN,\n",
    "} \n",
    "\n",
    "fixed_paramsDQ = {\n",
    "    'epoch':100, \n",
    "    'early_stopping':es, \n",
    "    'batch_size':40, \n",
    "    'lr':5e-4, \n",
    "    'kp':1, \n",
    "    'hiddens':1024, # 1024 for gating, 2048 for no gating\n",
    "    'Fsize':[2,2], # [2,2] for gating, [2,3] for no gating\n",
    "    'gating':True, \n",
    "    'bn':True, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def show_train_history(title, train, valid, earlystop=es):\n",
    "    epoch = len(train)\n",
    "    best = epoch-earlystop\n",
    "    x = [i for i in range(1, epoch + 1)]\n",
    "    plt.figure(figsize=(5,12))\n",
    "    ax = plt.figure().gca()\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.plot(x, train, marker='o', linestyle='-', color='b')\n",
    "    plt.plot(x, valid, marker='o', linestyle='-', color='r')\n",
    "    plt.axvline(best, color='black')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PATH = 'PickleResult/'\n",
    "# bestND = pickle.load(open(BEST_PATH + 'bestND.p', \"rb\"))\n",
    "bestDQAs = pickle.load(open(BEST_PATH + 'memoryDQAs.p', \"rb\"))\n",
    "bestDQSs = pickle.load(open(BEST_PATH + 'memoryDQSs.p', \"rb\"))\n",
    "bestDQEs = pickle.load(open(BEST_PATH + 'memoryDQEs.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (?, 7, 150, 100)\n",
      "CNNRNN|14|False|True|2_3|1024|256_512_1024|1|0.0246|0.0932\n"
     ]
    }
   ],
   "source": [
    "e = True\n",
    "for mr in [None]:\n",
    "    for fn in [[256,512,1024]]:\n",
    "        for num_layers in [2]:\n",
    "#                 trainXp = trainX[:int(train_len/10*prop)]\n",
    "#                 trainNDp = trainND[:int(train_len/10*prop)]\n",
    "#                 train_turnsp = train_turns[:int(train_len/10*prop)]\n",
    "#                 train_masksp = train_masks[:int(train_len/10*prop)]\n",
    "#                 dataND = [trainXp, trainNDp, train_turnsp, train_masksp, devX, devND, dev_turns, dev_masks, testX, test_turns, test_masks]\n",
    "\n",
    "            testname = 'ND_ablation_withGating'\n",
    "            testND, train_losses, dev_losses = stctrain.start_trainND(\n",
    "                *dataND, \n",
    "                **fixed_paramsND,\n",
    "                Fnum=fn, num_layers=num_layers, memory_rnn_type=None,\n",
    "                evaluate=e,\n",
    "            )\n",
    "\n",
    "#             show_train_history(testname, train_losses, dev_losses)\n",
    "            datahelper.pred_to_submission(testND, bestDQAs[0], bestDQSs[0], bestDQEs[0], test_turns, testIDs, filename='{}.json'.format(testname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(testND, open('bestND190116.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memoryNDs = pickle.load(open('PickleResult/memoryNDs.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainDQA, train_turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = True\n",
    "method = DQ.CNNCNN\n",
    "for l in [1]:\n",
    "    for rm in ['Bi-LSTM']:\n",
    "        for fn in [[512, 1024]]:\n",
    "            testname = 'ND_trainsize_{}perc'.format(prop*10)\n",
    "            print(testname, 'is started')\n",
    "        \n",
    "            bestDQA, train_lossesA, dev_lossesA = stctrain.start_trainDQ(\n",
    "                *dataDQA, \n",
    "                **fixed_paramsDQ, scoretype='DQA', method=method,\n",
    "                Fnum=fn, memory_rnn_type=rm, num_layers=l,\n",
    "                evaluate=e,\n",
    "            )\n",
    "            \n",
    "\n",
    "            bestDQE, train_lossesE, dev_lossesE = stctrain.start_trainDQ(\n",
    "                *dataDQE, \n",
    "                **fixed_paramsDQ, scoretype='DQE', method=method,\n",
    "                Fnum=fn, memory_rnn_type=rm, num_layers=l,\n",
    "                evaluate=e,\n",
    "            )\n",
    "\n",
    "            bestDQS, train_lossesS, dev_lossesS = stctrain.start_trainDQ(\n",
    "                *dataDQS, \n",
    "                **fixed_paramsDQ, scoretype='DQS', method=method,\n",
    "                Fnum=fn, memory_rnn_type=rm, num_layers=l,\n",
    "                evaluate=e\n",
    "            )\n",
    "       \n",
    "            datahelper.pred_to_submission(memoryNDs[0], bestDQA, bestDQS, bestDQE, test_turns, testIDs, filename='{}.json'.format(testname))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(bestDQAs, open('memoryDQAs.p', 'wb'))\n",
    "# pickle.dump(bestDQSs, open('memoryDQSs.p', 'wb'))\n",
    "# pickle.dump(bestDQEs, open('memoryDQEs.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_to_pred(path='ReTesting/0220_wordemb_test/NoneMemory_3stackCNN_2stackRNN(best).json'):\n",
    "    import json\n",
    "    with open(path) as f:\n",
    "        test_preds_json = json.load(f)\n",
    "        \n",
    "    pred = []\n",
    "    \n",
    "    for testID in testIDs:\n",
    "        for test_pred_json in test_preds_json:\n",
    "            _id = test_pred_json['id']\n",
    "            if _id != testID:\n",
    "                continue\n",
    "            dialogue_nuggets = test_pred_json['nugget']\n",
    "            dialogue_pred = [] \n",
    "            \n",
    "            for utterance_nugget in dialogue_nuggets:\n",
    "                utterance_pred = [None] * 7\n",
    "                if len(utterance_nugget.keys()) == 4:\n",
    "                    utterance_pred[0] = utterance_nugget['CNUG*']\n",
    "                    utterance_pred[1] = utterance_nugget['CNUG']\n",
    "                    utterance_pred[2] = utterance_nugget['CNaN']\n",
    "                    utterance_pred[3] = utterance_nugget['CNUG0']\n",
    "                    utterance_pred[4] = 0.\n",
    "                    utterance_pred[5] = 0.\n",
    "                    utterance_pred[6] = 0.\n",
    "                elif len(utterance_nugget.keys()) == 3:\n",
    "                    utterance_pred[0] = 0.\n",
    "                    utterance_pred[1] = 0.\n",
    "                    utterance_pred[2] = 0.\n",
    "                    utterance_pred[3] = 0.\n",
    "                    utterance_pred[4] = utterance_nugget['HNUG*']\n",
    "                    utterance_pred[5] = utterance_nugget['HNUG']\n",
    "                    utterance_pred[6] = utterance_nugget['HNaN']\n",
    "                \n",
    "                dialogue_pred.append(utterance_pred)\n",
    "                \n",
    "            while len(dialogue_pred) < 7:\n",
    "                dialogue_pred.append([0] * 7)\n",
    "                \n",
    "            pred.append(dialogue_pred)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testND = submission_to_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNDmasked = [np.multiply(nd, mask) for nd, mask in zip(testND, test_masks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDQA_NDF += [testNDmasked]\n",
    "dataDQE_NDF += [testNDmasked]\n",
    "dataDQS_NDF += [testNDmasked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataDQA_NDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = DQNDF.CNNCNN\n",
    "e = True\n",
    "testND = np.asarray(testND)\n",
    "\n",
    "# for prop in range(1, 11):\n",
    "for mr in ['Bi-LSTM']:\n",
    "    for fnum in [[512, 1024]]:\n",
    "        for num_layers in [1]:\n",
    "            testname = 'test'\n",
    "            print(testname, 'is started')\n",
    "\n",
    "#             trainXp = trainX[:int(train_len/10*prop)]\n",
    "#             train_turnsp = train_turns[:int(train_len/10*prop)]\n",
    "#             trainNDp = trainND[:int(train_len/10*prop)]\n",
    "\n",
    "#             trainDQAp = trainDQA[:int(train_len/10*prop)]      \n",
    "#             dataDQA_NDF = [trainXp, trainDQAp, train_turnsp, trainNDp, devX, devDQA, dev_turns, devND, testX, test_turns, testNDmasked]\n",
    "#             trainDQEp = trainDQE[:int(train_len/10*prop)]         \n",
    "#             dataDQE_NDF = [trainXp, trainDQEp, train_turnsp, trainNDp, devX, devDQE, dev_turns, devND, testX, test_turns, testNDmasked]\n",
    "#             trainDQSp = trainDQA[:int(train_len/10*prop)]              \n",
    "#             dataDQS_NDF = [trainXp, trainDQSp, train_turnsp, trainNDp, devX, devDQS, dev_turns, devND, testX, test_turns, testNDmasked]\n",
    "\n",
    "\n",
    "            bestDQNDFA, train_lossesA, dev_lossesA = stctrain.start_trainDQ_NDF(\n",
    "                *dataDQA_NDF, \n",
    "                **fixed_paramsDQ, scoretype='DQA', method=method,\n",
    "                Fnum=fnum, memory_rnn_type=mr, num_layers=num_layers,\n",
    "                evaluate=e, bert=False,\n",
    "            )\n",
    "\n",
    "            bestDQNDFE, train_lossesE, dev_lossesE = stctrain.start_trainDQ_NDF(\n",
    "                *dataDQE_NDF, \n",
    "                **fixed_paramsDQ, scoretype='DQE',\n",
    "                Fnum=fnum, method=method, memory_rnn_type=mr, num_layers=num_layers,\n",
    "                evaluate=e, bert=False,\n",
    "            )\n",
    "\n",
    "            bestDQNDFS, train_lossesS, dev_lossesS = stctrain.start_trainDQ_NDF(\n",
    "                *dataDQS_NDF, \n",
    "                **fixed_paramsDQ, scoretype='DQS', \n",
    "                Fnum=fnum, method=method, memory_rnn_type=mr, num_layers=num_layers,\n",
    "                evaluate=e, bert=False,\n",
    "            )\n",
    "\n",
    "            datahelper.pred_to_submission(testND, bestDQNDFA, bestDQNDFS, bestDQNDFE, test_turns, testIDs, filename='{}.json'.format(testname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(bestDQANDFs, open('memoryDQANDFs.p', 'wb'))\n",
    "# pickle.dump(bestDQSNDFs, open('memoryDQSNDFs.p', 'wb'))\n",
    "# pickle.dump(bestDQENDFs, open('memoryDQENDFs.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
